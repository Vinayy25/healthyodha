# HealthYoda: Agents SDK with Real-Time Tool Calling

## Overview

This implementation uses the **OpenAI Agents SDK** for TypeScript/JavaScript, which provides:

- ‚úÖ Real-time voice conversation (speech-to-speech)
- ‚úÖ **Tool calling during conversation** (the key feature we needed!)
- ‚úÖ Automatic tool execution and result injection
- ‚úÖ Session management and history
- ‚úÖ Better abstraction than raw WebRTC

## Architecture

### Components

```
Frontend (React + Agents SDK)
    ‚Üì
    Initializes RealtimeAgent with tools
    ‚Üì
Backend (Node.js)
    ‚îú‚îÄ /client-secret: Returns ephemeral token
    ‚îî‚îÄ /rag: Responds to RAG queries from tools
    ‚Üì
    /summary: Responds to summary generation from tools
    ‚Üì
AWS RAG Service (FAISS)
    ‚îî‚îÄ Medical handbook retrieval
    ‚Üì
OpenAI Realtime API
    ‚îî‚îÄ Processes voice + executes tools in real-time
```

### Data Flow

```
1. Frontend initializes RealtimeAgent
   ‚îú‚îÄ Define tools with Zod schemas
   ‚îî‚îÄ Set system instructions

2. User speaks symptom
   ‚Üì

3. OpenAI processes audio
   ‚îú‚îÄ Recognizes need for tool call
   ‚îî‚îÄ Automatically calls frontend tool

4. Frontend tool executes
   ‚îú‚îÄ Calls backend /rag endpoint
   ‚îú‚îÄ Gets medical framework
   ‚îî‚îÄ Returns to OpenAI

5. OpenAI generates next question
   ‚îú‚îÄ Uses medical framework from tool
   ‚îî‚îÄ Speaks answer to user

6. Repeat 2-5 until done

7. When ready, OpenAI calls generate_medical_summary tool
   ‚Üì

8. Summary tool calls backend /summary
   ‚îú‚îÄ Generates structured report
   ‚îî‚îÄ Displays to doctor
```

## Frontend Implementation

### Tool Definition (Zod Schema)

```typescript
import { tool } from "@openai/agents";
import { z } from "zod";

// Tool 1: RAG Retrieval
const getRelevantQuestions = tool({
  name: "get_relevant_questions",
  description: "Retrieve medical history questions based on symptoms",
  parameters: z.object({
    symptom: z.string().describe("Patient's chief complaint"),
  }),
  async execute({ symptom }) {
    const response = await fetch("http://localhost:3001/rag", {
      method: "POST",
      body: JSON.stringify({ query: symptom, k: 2 }),
    });
    const data = await response.json();
    return {
      context: data.context,
      sources: data.sources,
    };
  },
});

// Tool 2: Summary Generation
const generateMedicalSummary = tool({
  name: "generate_medical_summary",
  description: "Generate structured medical report",
  parameters: z.object({
    conversation: z.string(),
  }),
  async execute({ conversation }) {
    const response = await fetch("http://localhost:3001/summary", {
      method: "POST",
      body: JSON.stringify({ transcripts }),
    });
    const data = await response.json();
    return { summary: data.summary };
  },
});
```

### Agent Initialization

```typescript
import { RealtimeAgent, RealtimeSession } from "@openai/agents";

// Create agent with tools
const agent = new RealtimeAgent({
  name: "HealthYoda Medical Assistant",
  instructions: `You are HealthYoda...
    - Use get_relevant_questions tool to get medical framework
    - Use generate_medical_summary tool when done
  `,
  tools: [getRelevantQuestions, generateMedicalSummary],
});

// Get ephemeral token
const tokenResponse = await fetch("http://localhost:3001/client-secret");
const { client_secret } = await tokenResponse.json();

// Create and connect session
const session = new RealtimeSession(agent, {
  model: "gpt-4o-realtime-preview-2024-10-01",
  modalities: ["text", "audio"],
});

await session.connect({
  clientSecret: client_secret.value,
});

// Handle events
session.on("transcription.updated", (event) => {
  setTranscripts((prev) => [
    ...prev,
    {
      role: event.role,
      text: event.transcript,
    },
  ]);
});
```

## Backend Implementation

### New Endpoint: `/client-secret`

This endpoint returns an ephemeral token for the Agents SDK to use:

```javascript
app.get("/client-secret", async (req, res) => {
  const response = await fetch("https://api.openai.com/v1/realtime/sessions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${OPENAI_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "gpt-4o-realtime-preview-2024-10-01",
    }),
  });

  const data = await response.json();
  res.json({ client_secret: data.client_secret });
});
```

### Existing RAG Endpoint

The `/rag` endpoint now handles tool calls:

```javascript
app.post("/rag", async (req, res) => {
  const { query, k = 2 } = req.body;

  // Query AWS RAG service
  const ragResponse = await fetch(`${RAG_SERVICE_URL}/rag`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ query, k }),
  });

  const ragData = await ragResponse.json();

  // Return framework for tool
  res.json({
    context: ragData.context,
    sources: ragData.sources,
    num_chunks: ragData.num_chunks,
  });
});
```

## Key Differences from Previous Implementation

| Feature                  | Previous (Dual-Session) | New (Agents SDK)  |
| ------------------------ | ----------------------- | ----------------- |
| Tool Calling             | No (context embedded)   | ‚úÖ Yes (dynamic)  |
| Session Count            | 2 (greeting + context)  | 1 (continuous)    |
| Tool Execution           | Manual by backend       | Automatic by SDK  |
| Audio Handling           | Manual WebRTC           | Abstracted by SDK |
| Voice Activity Detection | Manual                  | ‚úÖ Built-in       |
| Transcription            | Manual data channel     | ‚úÖ Built-in       |
| Error Recovery           | Complex                 | ‚úÖ Simpler        |

## How Tool Calling Works

### During Conversation

```
User: "I have chest pain"
         ‚Üì
OpenAI (Realtime): "I should call get_relevant_questions to understand what to ask"
         ‚Üì
Frontend tool executes:
  - Fetches http://localhost:3001/rag?query=chest pain
  - Gets cardiac assessment framework
  - Returns to OpenAI
         ‚Üì
OpenAI: "Based on the framework, I'll ask about onset"
         ‚Üì
OpenAI Voice: "When did this chest pain start?"
```

### At End of Interview

```
OpenAI: "I've gathered sufficient information"
         ‚Üì
OpenAI calls: generate_medical_summary tool
         ‚Üì
Frontend tool executes:
  - Fetches http://localhost:3001/summary
  - Backend generates report using GPT-4o-mini
  - Returns to frontend
         ‚Üì
Report displayed to doctor
```

## Setup & Installation

### 1. Install dependencies

```bash
cd frontend
npm install @openai/agents zod
```

### 2. Ensure backend is running

```bash
cd backend
./start.sh
# Listens on http://localhost:3001
```

### 3. Ensure RAG service is running

```bash
cd rag_service
./start.sh
# Listens on http://localhost:3000
```

### 4. Start frontend

```bash
cd frontend
npm run dev
# Open http://localhost:5173
```

## Testing

### Test 1: Tool Execution During Conversation

1. Open frontend
2. Click "üéôÔ∏è Start Conversation"
3. Say: "I have chest pain"
4. Check browser console for:
   ```
   üîç Tool called: Retrieving questions for symptom: "chest pain"
   ‚úÖ RAG Tool Response: 2 chunks from 2 sources
   ```

### Test 2: Summary Generation

1. Have brief conversation
2. Listen for: "I have gathered sufficient information"
3. OpenAI calls summary tool
4. Check console for:
   ```
   üìù Tool called: Generating medical summary
   ‚úÖ Medical summary generated
   ```
5. Summary appears on screen

### Test 3: Backend Tool Endpoints

```bash
# Test RAG tool endpoint
curl -X POST http://localhost:3001/rag \
  -H "Content-Type: application/json" \
  -d '{"query":"chest pain","k":2}'

# Test client-secret endpoint
curl http://localhost:3001/client-secret
```

## Advantages of This Approach

‚úÖ **Real Tool Calling**: OpenAI decides when to call tools, not manual backend logic
‚úÖ **Single Session**: Cleaner, faster, no reconnection needed
‚úÖ **Automatic Transcription**: SDK handles speech-to-text
‚úÖ **Voice Activity Detection**: SDK handles pause detection
‚úÖ **Better Error Handling**: SDK manages connection state
‚úÖ **Cleaner Code**: Less manual WebRTC/audio management
‚úÖ **Production Ready**: OpenAI maintains the SDK
‚úÖ **Scalable**: Built on proven APIs

## Limitations & Considerations

‚ö†Ô∏è **SDK Maturity**: Agents SDK is newer, may have occasional issues
‚ö†Ô∏è **Tool Latency**: Tool calls add ~500ms (RAG retrieval time)
‚ö†Ô∏è **Network Dependency**: Requires stable internet for tool calls
‚ö†Ô∏è **Error Propagation**: If RAG fails, tool needs graceful fallback
‚ö†Ô∏è **Cost**: More API calls for tool execution (but more accurate)

## Troubleshooting

### Issue: Tool not calling

**Check**:

- Browser console for tool execution logs
- Backend logs for `/rag` endpoint calls
- OpenAI instructions mention using the tool

### Issue: Tool execution fails

**Check**:

- RAG service is running: `curl $RAG_SERVICE_URL/health` (where RAG_SERVICE_URL is from .env)
- Backend is accessible: `curl http://localhost:3001/rag`
- Tool parameters match Zod schema

### Issue: Audio not working

**Check**:

- Microphone permissions granted
- Browser console for audio errors
- Session.on("audio") events firing

## Next Steps

1. **Deploy to EC2**: Copy frontend build to EC2, update API URLs
2. **Add More Tools**: Implement additional medical tools
3. **Improve RAG**: Add more medical frameworks to handbook.txt
4. **Analytics**: Track tool call usage and accuracy
5. **Multi-language**: Add translation tool for non-English patients

## References

- [OpenAI Agents SDK - GitHub Pages](https://openai.github.io/openai-agents-js/)
- [Building Voice Agents - Full Guide](https://openai.github.io/openai-agents-js/guides/voice-agents/build/)
- [Realtime API Documentation](https://platform.openai.com/docs/guides/realtime-voice-agents)
- [Zod Validation Library](https://zod.dev)

---

**This implementation provides true real-time tool calling for medical knowledge retrieval, making HealthYoda a fully intelligent medical interview assistant!** üöÄ
